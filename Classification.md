<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
# 模式识别笔记——分类(Classification)

作者：李一肖

原文地址：

## 1 简介

#### **分类的流程**：

- 有一个未知类别的，维数(特征数)为 $n$ 的样本 $X(X \in \mathbb{R^n})$ 

- 现有$c$类，$\omega_i, \omega_2,...,\omega_c$
- 每类有一个分类器(函数)$g_i(X)$，他把一个维数为$n$的样本映射到一维：$ \mathbb{R^n \rightarrow R}$

- 让样本$X$经过所有分类器$g_i(X)$，得到$c$个分数(函数值)，$g_1(X),g_2(X),...,g_k(X),...,g_c(X)$

- 选择最高(低)分数对应的分类器$g_k(X)$，那么样本$X$属于第$k$类

#### **上述分类流程中待解决的问题**：

问：如何得到分类器(函数)或者如何设计分类器？

答：结构+参数，即主要矛盾+次要矛盾

结构是指：采取怎样的判别方法，即怎样的**函数结构**。这个的选取要根据分类问题(样本的分布)的具体要求设定。例如，我们神奇地(之后可以通过计算)知道这是一个线性可分的样本，于是我们选择线性函数。

参数是指：选定函数之后，这个函数的参数。例如，线性函数对应的权重是一些列参数，高斯函数的均值和方差等。

于是上面的问题又分为两个子问题：

（1）问：如何选取合适的判别方法，即合适的函数？

​          答：大致有两种：**基于几何的方法，基于概率的方法**

（2）问：如何得到(训练、学习)参数？

​		  答：具体函数具体分析。但大多都是基于监督学习，即从大量已知类别的样本中学习参数。

## 2 基于几何的方法

### 2.1 判别函数

下面我们介绍的判别函数，都默认参数是已知。至于这些参数通过什么算法得到，我们放到之后2.2 参数学习。事实上，有些参数可以显式地由样本表示，比如势函数法；但有些需要通过复杂的迭代，比如线性判别函数。

#### 2.1.1 线性判别函数

问题的描述在简介中提到过，这里不赘述。下式是线性判别函数
$$
\underset{arg\ i}{max} \ g_i(X) = w_i^TX +b_i
$$
参数：$w_i$是第$i$个分类器的权重，有$n$维；$b_i$是bias，

有时引入增广向量，简写为：
$$
g_i(X) = w_i^TX
$$
注意这里的$X = [X,1]$，是$n+1$维的增广向量

#### 2.1.2 非线性判别函数——距离判别

函数结构：
$$
\underset{i=1,2,...,c}{min} \ \ g_i(X) =  ||X-\mu_i||
$$
参数：$\mu_i$是第$i$类的均值。

这个判别函数的结构几何意义是：把样本分到最近的一个类。这个距离可以是L2距离，对应的$\mu_i$就是该类的几何中心。

#### 2.1.2 非线性判别函数——二次函数判别

$$
\underset{arg\ i}{min}\ g_i(X) = X^TD_i X + c_i^T X +b
$$

参数：$D_i$是实二次型方阵，它其实是第$i$类空间的协方差矩阵（见后面Basey分类器），$c_i$是$n$维向量，它其实是第$i$类空间的均值向量，$b$是bias

说明：令分类器=0，若在2维空间得到的曲线可能是：圆、椭圆、双曲线、抛物线；3维类则有球、椭球、圆锥面，鞍面等。

#### 2.1.3 非线性判别函数——势函数判别

$$
\underset{arg\ i}{max}\ g_i(X) = \sum_{X_k\in \omega_i} V(X, X_k)
$$

其中$V$的选取有很多，比如：
$$
\begin{aligned}
V(X, X_k) &= e^{-\alpha|X-X_k|} \\
V(X, X_k) &= \frac{1}{1+\alpha|X-X_k|} \\
V(X, X_k) &= \frac{\sin(\alpha|X-X_k|^2)}{\alpha|X-X_k|^2}
\end{aligned}
$$
参数：$\alpha$，各已知类别的样本$X_k$也是参数



### 2.2 参数学习

#### 2.2.1 线性判别函数的参数学习

- **梯度下降算法（迭代法）**

$$
w^{(k+1)}_i = w_i^{(k)} - \rho^{(k)} \nabla J(w_i^{(k)})
$$

其中$J(w^{(k)}) = w^{(k)\ T} X^{(k)}$，$w$才是自变量，$X^{(k)}$是第$k$个样本。一个比较好的$\rho^{(k)} = \nabla^2J(w^{(k)})^{-1}$，是$J(w)$二阶偏导数的逆

- **感知器算法——赏罚分类器**

  思想：所有判别器算出分数后，给错误的分类器的权重加上正比于样本$X^{(k)}$的量

  算法：

  - $k=0$时，所有线性判别器的初始权重$w_i^{(0)}$随机赋值，比如全部置零
  - 输入训练样本$X^{(k)}$，它属于第$m$类，感知器计算分数（$g_m(X^{(k)})$应该是最大的）
  - 比较$g_m(X^{(k)})$和其他所有$g_i(X^{(k)})$分数的大小，这里$i=1,2,...,c,\neq m$
    - 若$g_m(X^{(k)})$最大，即$g_m(X^{(k)}) > g_i(X^{(k)})，i\neq m$，则$w_i^{(k+1)}=w_i^{(k)},i=1,...,c$，都保持不动
    - 若$g_m(X^{(k)})$不是最大，奖励$w_m^{(k+1)}$让它变得更大，即$w_m^{(k+1)} =w_m^{(k)}+ \rho X^{(k)} $；并且惩罚那些比$g_m(X^{(k)})$大的分类器的权重$w_j^{(k+1)}$让它们变得更小，即$w_j^{(k+1)} =w_j^{(k)} - \rho X^{(k)}$

  - 循环遍历所有样本，直到所有$g_i(X^{(k)})$不再变化

- **MSE准则（非迭代法）——解超定线性方程**



#### 2.2.2 判断是否线性可分——H-K算法

假设有$m$个训练样本，把它们按列排成$n\times m$的矩阵$X,X = [X^{(1)},X^{(2)}, ... ,X^{(m)}]$

* 求伪逆矩阵$X^{\#} = (X^TX)^{-1}X^T$

- 赋初值$b^{(0)}$，只要$b^{(0)}>0$即可，比如$b^{(0)}=[1,1, ..., 1]^T$
- 计算$w^{(k)} = X^{\#}b^{(k)}$，$e^{(k)} = Xw^{(k)} - b^{(k)}$
- 判断$e^{(k)}$中个元素的正负
  - 若有负值或者收敛后(迭代多次后个元素不变)有正值，则线性不可分
  - 若全部都是0，或者是无穷小量，则线性可分
  - 若都不是，则继续按下面的算法迭代

- $w^{(k+1)} = w^{(k)} + \rho X^{\#}｜e^{(k)}｜$，$b^{(k+1)} = b^{(k)}+\rho(e^{(k)}+|e^{(k)}|)$，element-wise absolute

#### 2.2.3 非线性判别函数的参数学习——略







## 3 基于概率的方法

### 3.1 判别函数(准则)

Notations：

- 类别$\omega_i$的先验分布：$P(\omega_i)$
- 已知在类别$\omega_i$下，观察到$X$也属于$\omega_i$的条件概率：$P(X|\omega_i)$
- 在观测到样本$X$属于类$\omega_i$后，$\omega_i$的后验分布：$P(\omega_i|X)$

具体的意义就不解释了，参考《概率论与数理统计》，陈希孺

#### 3.1.1 最小错误判决规则（最大后验概率）

已知所有类别$\omega_i$的先验分布$P(\omega_i)$，也已知条件概率$P(X|\omega_i)$

“最小错误”中的错误指的是：第一类错误和第二类错误的总和
$$
\underset{arg\ i }{max}\ g_i(X) = P(\omega_i|X) = \frac{P(X|\omega_i)P(\omega_i)}{\sum^c_{i=1} P(X|\omega_i)P(\omega_i)}
$$
同样的，如何得到先验分布和条件概率我们放到后面的参数估计和非参数估计介绍

#### 3.1.2 最小风险判决规则（最小损失，最大收益）

引入损失（收益）函数$\lambda(\alpha_i/\omega_j) $，其中$\alpha_i$是决策，指把样本$X$分类到类别$\omega_j$的损失

|                | $\omega_1$         | $\omega_2$         | ...  | $\omega_c$         |
| :------------: | ------------------ | ------------------ | ---- | ------------------ |
|   $\alpha_1$   | $\lambda_{11} $    | $\lambda_{12} $    |      | $\lambda_{1c} $    |
|   $\alpha_2$   | $\lambda_{21} $    | $\lambda_{22} $    |      | $\lambda_{2c} $    |
|      ...       |                    |                    |      |                    |
|   $\alpha_c$   | $\lambda_{c1} $    | $\lambda_{c2} $    |      | $\lambda_{cc} $    |
| $\alpha_{c+1}$ | $\lambda_{c+1,1} $ | $\lambda_{c+1,2} $ |      | $\lambda_{c+1,c} $ |

一般有$c$个分类就有$c$个决策，有时也引入“拒判”作为第$c+1$个决策。这个损失值是人为规定的，参数学习也学不到的。

显而易见的，我们的分类目标就是找到给定随机样本$X$的条件下，期望损失最小的决策：
$$
\underset{arg\ i}{min}\ g_i(X) = E(\alpha_i|X) = \sum_{j=1}^c \lambda_{ij}P(\omega_j|X)=...
$$
...指的是用Bayes公式展开。

参数：同样的，先验分布和条件概率都是我们的参数；而损失函数则是非参数

#### 3.1.3 最小最大判决规则（悲观主义原则）

终于，我们不假设已知先验分布$P(\omega_i)$，仅仅假设已知条件概率$P(X|\omega_i)$，但又神奇的假设样本空间的划分$\Omega_i$是已知的（后面可以看到，这条假设在最小化总风险时可以不要）。

**首先解题："最大"指最大化每个决策的损失期望；"最小"指最小化总的风险。**

**我们从外向里推到：先最小，此时要在已知先验分布的情况下推导；再最大，这时消去先验分布。**

在上面最小化风险判决中，我们可以计算出每个决策的期望损失$E(\alpha_i|X)$，于是我们可以计算总的期望损失，它依赖于$X$的分布$P(X)$（可用全概率公式展成条件概率$P(X|\omega_i)$和先验分布$P(\omega_i)$）
$$
R = \int_{\Omega_1}E(\alpha_1|X)P(X)dX + \int_{\Omega_2}E(\alpha_2|X)P(X)dX+...+\int_{\Omega_c}E(\alpha_c|X)P(X)dX
$$
我们下面只研究2个类别的情况：
$$
R = a+bP(\omega_1)
$$
其中，
$$
\begin{aligned}
a =& \lambda_{22} + (\lambda_{12} -  \lambda_{22} )\int_{\Omega_1}P(X|\omega_2)dX \\
b =& (\lambda_{11} - \lambda_{12}) + (\lambda_{21}-\lambda_{11})\int_{\Omega_2}P(X|\omega_1)dX-(\lambda_{12} -  \lambda_{22} )\int_{\Omega_1}P(X|\omega_2)dX
\end{aligned}
$$
可以看到总的期望风险跟先验分布$P(\omega_1)$有关；同时也跟空间的划分$\Omega_1,\Omega_2$有关，这跟分类器的选择有关（a，b不是常数）。下面假设选择最小风险判决的分类器，得到$\Omega_1,\Omega_2$（在假设已知先验分布后，可通过令$g_1(X)=g_2(X)$，这个曲线就是分界线），而这个$\Omega_1,\Omega_2$也直接和先验分布挂钩。

理论上，给定$P(\omega_1)$我们就可以得到最小总风险的期望，记做$R_{min}$，这是由于我们采取最小风险判决得到$\Omega_1,\Omega_2$的缘故。事实上，只要$\Omega_1,\Omega_2$由最小风险判决得到，即通过令$g_1(X)=g_2(X)$，上面的式子可以写为：
$$
R_{min} = a+bP(\omega_1)
$$
再次强调，$a,b$不是常数，所以这是一条单峰的、上凹函数如下图。



<img src="/Users/yxli/Library/Application Support/typora-user-images/Screen Shot 2020-12-30 at 09.46.45.png" alt="Screen Shot 2020-12-30 at 09.46.45" style="zoom:33%;" />

由于我们不知道先验分布$P(\omega_1)$，所以我们只能做最坏的打算。也就是说，在先验分布$P(\omega_1)$的变化过程中中选择$R_{min}$最大的值，记做$R_{min,max}$，即上图的极点。

**可以证明，先最大化每个$P(\omega_1)$对应的决策的损失期望，再最小化总的风险，等价于先最小化总的风险（是$P(\omega_1)$的函数），再最大化这个函数。**即
$$
\underset{arg\ i}{min}\ \underset{\alpha_j}{max}\ R = \underset{P(\omega_1)}{max}\ R_{min} = R_{min,max}
$$
总结设计步骤：

- 画出$R_{min} = a+bP(\omega_1)$的曲线，找到最大值对应的坐标$(P_{min,max}(\omega_1),R_{min,max})$
- 根据找到的这个$P_{min,max}(\omega_1)$就是最悲观的先验分布，这时我们已知先验分布、条件概率了，问题转化为3.1.2最小风险判决
- 后面判决同3.1.2最小风险判决

#### 3.1.4 Neyman-Peason判决

还是从3.1.2的最小风险判决发展而来。3.1.2的最小风险判决的损失函数需要人为规定，不方便使用。

**于是N-P判决采用的思想是：固定犯第一类的错误的概率，最小化犯第二类错误的概率；或者固定犯第二类的错误的概率，最小化犯第一类错误的概率。**

#### 3.1.5 最小错误判决规则的实例

在3.1.1中我们介绍了最小错误判决规则。那里我们没有讲各个分布函数的具体形式，这里我们以一类特殊的分布函数——正态分布为例，介绍一下具体的分类函数$g_i(X)$的样子

已知条件概率为正态分布，但参数$\mu_i,\Sigma_i$需要学习
$$
P(X|\omega_i) = \frac{1}{\sqrt{(2\pi)^n\det(\Sigma_i)}}e^{-\frac{1}{2}(X-\mu_i)^T\Sigma_i^{-1}(X-\mu_i)}
$$
参数$\mu_i$是类空间的均值，是和$X$维数一样的$n$维向量；参数$\Sigma_i$是类空间的协方差矩阵，是$n\times n$方阵

由于参数需要学习得到，我们就依次讨论参数可能出现的情况（分3种情况）：

##### （1）第一种情况：各特协方差相等，且是对角矩阵

##### （2）第二种情况：各类协方差相等，且与$i$无关，但不是对角矩阵

##### （3）第三种情况：各类协方差不相等



### 3.2 参数学习

上面我们看到，先验分布$P(\omega_i)$和条件分布$P(X|\omega_i)$都是未知的。我们甚至不知道这些概率分布函数具体的函数形式。于是我们有2条路可以走：

- 假设这些分布函数都是常见的，比如正态分布、指数分布、二项分布等；然后学习对应分布函数的参数，比如正态分布里面的$\mu,\sigma$，指数分布里面的$\mu$
- 直接通过训练样本， 拟合出近似分布

前者叫参数估计，后者叫非参数估计

#### 3.2.1 参数估计

- 极大似然估计（见《概率论与数理统计》，陈希孺）
- Bayes估计（见《概率论与数理统计》，陈希孺）
- （矩估计）

#### 3.2.2 非参数估计

我们的任务是估计概率密度函数，那我们如何估计呢？以一维为例，我们知道$P(X\leq x) =\int_{-\infty}^{x}p(x)dx $

那么概率密度就是概率比上体积：
$$
\hat{p}(x) = \frac{P(X\leq x+\Delta x)-P(X\leq x)}{\Delta x} = \frac{P(x\leq X\leq x+\Delta x)}{\Delta x}
$$
在这个估计中分母$\Delta x$是好确定的，分子$P(x\leq X\leq x+\Delta x)$如何得到呢？我们通过试验估计得到，也就是大数定律
$$
P(x\leq X\leq x+\Delta x)\approx \frac{k}{n}
$$
$k$是落入区间$[x,x+\Delta x]$的样本数，$n$是参与试验的总样本数。估计的概率密度又可以写成：
$$
\hat{p}(x) = \frac{k/n}{\Delta x}
$$
我们看到，当确定总的样本数$n$后，即做完$n$次试验后，$\Delta x$和$k$有2种方法确定：

（1）固定区间长度$\Delta x$，数有多少样本落入$[x,x+\Delta x]$，落进去的数量就是$k$;

（2）固定落入$x$邻域的样本数$k$，以$x$为中心，向两边增大区间长度$\Delta x$，直到$[x,x+\Delta x]$包含了$k$个样本，记下此时的区间长度$\Delta x$

前者叫Parzen窗口法，后者叫$K_n$近邻法，下面具体介绍

（1）Parzen窗口法

从上面的分析我们可以看到，Parzen窗口法的核心就是计数，那么多远的距离才算进来呢？聪明的读者可能想到：只要样本落进这个区间，计数器就加1不就好啦。但是，我们又想：离中心$x$近的和离它远的样本，我们都一视同仁真的好吗？或者我们可以给离得近的样本计0.9，给离得远的样本计0.4，然后把这些值加起来，得到总的计数$k$，于是我们可以构造不同的窗函数，给这些样本的计数加权。常见的窗函数如下：



<img src="/Users/yxli/Library/Application Support/typora-user-images/Screen Shot 2020-12-30 at 12.24.00.png" alt="Screen Shot 2020-12-30 at 12.24.00" style="zoom:35%;" />

$u$就是样本$X$到中心$x$的归一化距离，即
$$
u = \frac{|X-x|}{\Delta x}
$$
（2）$K_n$近邻法

这歌方法的核心是确定区间长度$\Delta x$，我们做了$n$次试验，有$X_1, X_2, ..., X_n$个独立同分布的样本。我们从中心$x$出发，向它周围依次数最近的$k$个样本（因此得名），不妨记做$X_1, X_2, ...,X_k$

区间长度就是
$$
\Delta x = max\{X_1, X_2, ..., X_k\} - min\{X_1, X_2, ..., X_k\}
$$

## 4 总结与拓展

基于几何分类和基于概率分类的共同点都是找到一个分类器，不同的是背后的假设——几何分类认为（训练和测试）样本的类别是确定的，只是分类前不知道；概率分类则认为（训练和测试）样本的类别是有概率分布的，我们只能尽可能正确分类。

在实际应用中，比如现在流行的图像分类，都是以概率分类为理论基础。

下面介绍一些拓展的分类方法以供参考

- 支持向量机SVM
- 人工神经网络ANN

- XGBoost
- ……（欢迎补充）
